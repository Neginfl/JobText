{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc5cb75-077c-4fdb-ade2-f5e2a78be4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job data available.\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating file handler\n",
    "log_handler = RotatingFileHandler('jobinja_scraper.log', maxBytes=5*1024*1024, backupCount=2)\n",
    "log_handler.setLevel(logging.ERROR)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v3\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            subpage_links = Queue()\n",
    "            unique_links = set()\n",
    "\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if full_url not in unique_links:\n",
    "                    subpage_links.put(full_url)\n",
    "                    unique_links.add(full_url)\n",
    "\n",
    "            print(f\"Found {len(unique_links)} unique links.\")  # Debugging line\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred while getting links: {e}\")\n",
    "            return Queue()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        job_features = {}\n",
    "\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text()) if job_title_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')\n",
    "        job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text()) if job_description_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        additional_features = {\n",
    "            'job_category': ('div', 'job-category'),\n",
    "            'job_location': ('div', 'job-location'),\n",
    "            'employment_type': ('div', 'employment-type'),\n",
    "            'min_experience': ('div', 'min-experience'),\n",
    "            'salary': ('div', 'salary'),\n",
    "            'gender': ('div', 'gender'),\n",
    "            'military_status': ('div', 'military-status'),\n",
    "            'education_level': ('div', 'education-level'),\n",
    "            'company_intro': ('div', 'company-intro'),\n",
    "            'skills_required': ('div', 'skills-required'),\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "            job_features[feature] = self.clean_persian_text(feature_tag.get_text()) if feature_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "        \n",
    "        print(f\"Extracted features: {job_features}\")  # Debug: print extracted features\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        while not subpage_links.empty():\n",
    "            subpage_url = subpage_links.get()\n",
    "            try:\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                if job_data['job_title'] != \"اطلاعات موجود نیست\":  # Ensure job title exists before saving\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save each job's features in a separate .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    print(f\"Saved job data to {file_path}\")  # Print confirmation of saving the file\n",
    "\n",
    "                time.sleep(1)\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "\n",
    "    def save_dataset(self):\n",
    "        mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "        json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "        job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "        \n",
    "        if job_data_dict:\n",
    "            sio.savemat(mat_file_path, job_data_dict)\n",
    "            with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def display_dataset(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if df.empty:\n",
    "            print(\"No job data available.\")\n",
    "        else:\n",
    "            print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jobinja_scraper = Jobinja()\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    jobinja_scraper.save_dataset()\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c48f50-0701-443f-8f77-0b94d3cee63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job data to save.\n",
      "No job data available.\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating file handler\n",
    "log_handler = RotatingFileHandler('jobinja_scraper.log', maxBytes=5*1024*1024, backupCount=2)\n",
    "log_handler.setLevel(logging.ERROR)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v3\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            subpage_links = Queue()\n",
    "            unique_links = set()\n",
    "\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if full_url not in unique_links:\n",
    "                    subpage_links.put(full_url)\n",
    "                    unique_links.add(full_url)\n",
    "\n",
    "            print(f\"Found {len(unique_links)} unique links.\")  # Debugging line\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred while getting links: {e}\")\n",
    "            return Queue()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        job_features = {}\n",
    "\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text()) if job_title_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')\n",
    "        job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text()) if job_description_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        additional_features = {\n",
    "            'job_category': ('div', 'job-category'),\n",
    "            'job_location': ('div', 'job-location'),\n",
    "            'employment_type': ('div', 'employment-type'),\n",
    "            'min_experience': ('div', 'min-experience'),\n",
    "            'salary': ('div', 'salary'),\n",
    "            'gender': ('div', 'gender'),\n",
    "            'military_status': ('div', 'military-status'),\n",
    "            'education_level': ('div', 'education-level'),\n",
    "            'company_intro': ('div', 'company-intro'),\n",
    "            'skills_required': ('div', 'skills-required'),\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "            job_features[feature] = self.clean_persian_text(feature_tag.get_text()) if feature_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "        \n",
    "        print(f\"Extracted features: {job_features}\")  # Debug: print extracted features\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        while not subpage_links.empty():\n",
    "            subpage_url = subpage_links.get()\n",
    "            try:\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                if job_data['job_title'] != \"اطلاعات موجود نیست\":  # Ensure job title exists before saving\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save each job's features in a separate .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    print(f\"Saved job data to {file_path}\")  # Print confirmation of saving the file\n",
    "                else:\n",
    "                    print(f\"No valid job title for URL: {subpage_url}\")  # Debug message for invalid title\n",
    "\n",
    "                time.sleep(1)  # Add delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "                print(f\"Request failed for URL: {subpage_url}. Error: {e}\")  # Print error for visibility\n",
    "\n",
    "    def save_dataset(self):\n",
    "        if self.all_jobs_data:\n",
    "            mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "            json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "            job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "            sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "            with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "            print(\"Dataset saved successfully.\")  # Confirmation message for saved dataset\n",
    "        else:\n",
    "            print(\"No job data to save.\")  # Informative message if no data\n",
    "\n",
    "    def display_dataset(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if df.empty:\n",
    "            print(\"No job data available.\")\n",
    "        else:\n",
    "            print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jobinja_scraper = Jobinja()\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    jobinja_scraper.save_dataset()\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "814a8a63-8f45-4ca7-85b1-8c87aea927e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job data to save.\n",
      "No job data available.\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating file handler\n",
    "log_handler = RotatingFileHandler('jobinja_scraper.log', maxBytes=5*1024*1024, backupCount=2)\n",
    "log_handler.setLevel(logging.ERROR)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v3\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            subpage_links = Queue()\n",
    "            unique_links = set()\n",
    "\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if full_url not in unique_links:\n",
    "                    subpage_links.put(full_url)\n",
    "                    unique_links.add(full_url)\n",
    "\n",
    "            print(f\"Found {len(unique_links)} unique links.\")  # Debugging line\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred while getting links: {e}\")\n",
    "            return Queue()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        job_features = {}\n",
    "\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text()) if job_title_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')\n",
    "        job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text()) if job_description_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        additional_features = {\n",
    "            'job_category': ('div', 'job-category'),\n",
    "            'job_location': ('div', 'job-location'),\n",
    "            'employment_type': ('div', 'employment-type'),\n",
    "            'min_experience': ('div', 'min-experience'),\n",
    "            'salary': ('div', 'salary'),\n",
    "            'gender': ('div', 'gender'),\n",
    "            'military_status': ('div', 'military-status'),\n",
    "            'education_level': ('div', 'education-level'),\n",
    "            'company_intro': ('div', 'company-intro'),\n",
    "            'skills_required': ('div', 'skills-required'),\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "            job_features[feature] = self.clean_persian_text(feature_tag.get_text()) if feature_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "        \n",
    "        print(f\"Extracted features: {job_features}\")  # Debug: print extracted features\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        while not subpage_links.empty():\n",
    "            subpage_url = subpage_links.get()\n",
    "            try:\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Debug: Check if job_data has a valid title\n",
    "                if job_data['job_title'] and job_data['job_title'] != \"اطلاعات موجود نیست\":\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save each job's features in a separate .txt file\n",
    "                    job_title = job_data.get('job_title', 'JobInja')\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    print(f\"Saved job data to {file_path}\")  # Print confirmation of saving the file\n",
    "                else:\n",
    "                    print(f\"No valid job title for URL: {subpage_url}\")  # Debug message for invalid title\n",
    "\n",
    "                time.sleep(1)  # Add delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "                print(f\"Request failed for URL: {subpage_url}. Error: {e}\")  # Print error for visibility\n",
    "\n",
    "    def save_dataset(self):\n",
    "        if self.all_jobs_data:\n",
    "            mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "            json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "            job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "            sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "            with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "            print(\"Dataset saved successfully.\")  # Confirmation message for saved dataset\n",
    "        else:\n",
    "            print(\"No job data to save.\")  # Informative message if no data\n",
    "\n",
    "    def display_dataset(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if df.empty:\n",
    "            print(\"No job data available.\")\n",
    "        else:\n",
    "            print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jobinja_scraper = Jobinja()\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    jobinja_scraper.save_dataset()\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f61152-21c5-4413-86bd-f5727c76b593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No job data to save.\n",
      "No job data available.\n",
      "No data available to generate statistics.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import json\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import pandas as pd\n",
    "from queue import Queue\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating file handler\n",
    "log_handler = RotatingFileHandler('jobinja_scraper.log', maxBytes=5*1024*1024, backupCount=2)\n",
    "log_handler.setLevel(logging.ERROR)\n",
    "log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "log_handler.setFormatter(log_formatter)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "logger.addHandler(log_handler)\n",
    "\n",
    "class Jobinja:\n",
    "    def __init__(self, base_url=\"https://jobinja.ir/\", save_dir=\"JobInja/v3\"):\n",
    "        self.base_url = base_url\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        }\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        self.all_jobs_data = []\n",
    "        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "    def get_links(self):\n",
    "        try:\n",
    "            response = requests.get(self.base_url, headers=self.headers, verify=False, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            links = soup.find_all('a', href=True)\n",
    "            subpage_links = Queue()\n",
    "            unique_links = set()\n",
    "\n",
    "            for link in links:\n",
    "                href = link['href']\n",
    "                if href.startswith('/'):\n",
    "                    full_url = self.base_url + href\n",
    "                elif href.startswith('http'):\n",
    "                    full_url = href\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if full_url not in unique_links:\n",
    "                    subpage_links.put(full_url)\n",
    "                    unique_links.add(full_url)\n",
    "\n",
    "            print(f\"Found {len(unique_links)} unique links.\")  # Debugging line\n",
    "            return subpage_links\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"An error occurred while getting links: {e}\")\n",
    "            return Queue()\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_persian_text(text):\n",
    "        cleaned_text = re.sub(r'[^؀-ۿ\\s]', ' ', text)\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "        return cleaned_text.strip()\n",
    "\n",
    "    def extract_job_features(self, subpage_soup):\n",
    "        job_features = {}\n",
    "\n",
    "        job_title_tag = subpage_soup.find('h1')\n",
    "        job_features['job_title'] = self.clean_persian_text(job_title_tag.get_text()) if job_title_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_description_tag = subpage_soup.find('div', class_='job-description')\n",
    "        job_features['job_description'] = self.clean_persian_text(job_description_tag.get_text()) if job_description_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        additional_features = {\n",
    "            'job_category': ('div', 'job-category'),\n",
    "            'job_location': ('div', 'job-location'),\n",
    "            'employment_type': ('div', 'employment-type'),\n",
    "            'min_experience': ('div', 'min-experience'),\n",
    "            'salary': ('div', 'salary'),\n",
    "            'gender': ('div', 'gender'),\n",
    "            'military_status': ('div', 'military-status'),\n",
    "            'education_level': ('div', 'education-level'),\n",
    "            'company_intro': ('div', 'company-intro'),\n",
    "            'skills_required': ('div', 'skills-required'),\n",
    "        }\n",
    "\n",
    "        for feature, (tag, class_name) in additional_features.items():\n",
    "            feature_tag = subpage_soup.find(tag, class_=class_name)\n",
    "            job_features[feature] = self.clean_persian_text(feature_tag.get_text()) if feature_tag else \"اطلاعات موجود نیست\"\n",
    "\n",
    "        job_features['content_snippet'] = self.clean_persian_text(subpage_soup.get_text())\n",
    "        \n",
    "        print(f\"Extracted features: {job_features}\")  # Debug: print extracted features\n",
    "        return job_features\n",
    "\n",
    "    def scrape_jobs(self):\n",
    "        subpage_links = self.get_links()\n",
    "\n",
    "        while not subpage_links.empty():\n",
    "            subpage_url = subpage_links.get()\n",
    "            try:\n",
    "                subpage_response = requests.get(subpage_url, headers=self.headers, verify=False, timeout=10)\n",
    "                subpage_response.raise_for_status()\n",
    "                subpage_soup = BeautifulSoup(subpage_response.text, 'html.parser')\n",
    "                \n",
    "                job_data = self.extract_job_features(subpage_soup)\n",
    "\n",
    "                # Only append if job data is valid and title is not empty\n",
    "                if job_data['job_title'] and job_data['job_title'] != \"اطلاعات موجود نیست\":\n",
    "                    self.all_jobs_data.append(job_data)\n",
    "\n",
    "                    # Save each job's features in a separate .txt file\n",
    "                    job_title = re.sub(r'[<>:\"/\\\\|?*]', '_', job_data['job_title'])  # Clean job title for file naming\n",
    "                    file_path = os.path.join(self.save_dir, f\"{job_title}.txt\")\n",
    "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                        for key, value in job_data.items():\n",
    "                            file.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "\n",
    "                    print(f\"Saved job data to {file_path}\")  # Print confirmation of saving the file\n",
    "                else:\n",
    "                    print(f\"No valid job title for URL: {subpage_url}\")  # Debug message for invalid title\n",
    "\n",
    "                time.sleep(1)  # Add delay to avoid overwhelming the server\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.error(f\"An error occurred while accessing {subpage_url}: {e}\")\n",
    "                print(f\"Request failed for URL: {subpage_url}. Error: {e}\")  # Print error for visibility\n",
    "\n",
    "    def save_dataset(self):\n",
    "        if self.all_jobs_data:\n",
    "            mat_file_path = os.path.join(self.save_dir, \"jobinja_data.mat\")\n",
    "            json_file_path = os.path.join(self.save_dir, \"jobinja_data.json\")\n",
    "\n",
    "            job_data_dict = {f\"job_{i}\": job for i, job in enumerate(self.all_jobs_data)}\n",
    "            sio.savemat(mat_file_path, job_data_dict)\n",
    "\n",
    "            with open(json_file_path, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(self.all_jobs_data, json_file, ensure_ascii=False, indent=4)\n",
    "            print(\"Dataset saved successfully.\")  # Confirmation message for saved dataset\n",
    "        else:\n",
    "            print(\"No job data to save.\")  # Informative message if no data\n",
    "\n",
    "    def display_dataset(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if df.empty:\n",
    "            print(\"No job data available.\")\n",
    "        else:\n",
    "            print(df)\n",
    "        return df\n",
    "\n",
    "    def descriptive_statistics(self):\n",
    "        df = pd.DataFrame(self.all_jobs_data)\n",
    "        if not df.empty:\n",
    "            print(df.describe(include='all'))\n",
    "        else:\n",
    "            print(\"No data available to generate statistics.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    jobinja_scraper = Jobinja()\n",
    "    jobinja_scraper.scrape_jobs()\n",
    "    jobinja_scraper.save_dataset()\n",
    "    df = jobinja_scraper.display_dataset()\n",
    "    jobinja_scraper.descriptive_statistics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9526cd-f9d2-4d8b-8dda-0326e3764ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
